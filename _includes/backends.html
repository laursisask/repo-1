<h1><a id="backends">Terraform Backends</a></h1>
<p>
    By default, Terraform stores the state of any resources it creates in local files. This is called the local backend, and it is convenient for quickly getting up and running with Terraform as it requires no additional configuration.
</p>
<p>
    However, using the local backend means that you must run Terraform from the same machine every time. If you try to reapply Terraform modules from a second machine, the second machine does not have access to the state of the created resources, and the process will fail saying that the resources defined in the Terraform configuration already exist.
</p>
<p>
    It is highly recommended that Terraform be configured with a shared remote state. Doing so allows the Terraform module to be applied from any location.
</p>
<p>
    Terraform supports storing state in S3 buckets. Each of the directories under the <strong>terraform</strong> directory has a child directory called <strong>s3backend</strong>. The <strong>s3backend</strong> module contains a file called <strong>backend.tf</strong> which configures an S3 backend:
</p>
<pre>terraform {
  backend "s3" {    
  }
}</pre>
<p>
    The details of the provider must be defined in the top level module:
</p>
<pre>terraform {
    required_providers {
      octopusdeploy = { source = "OctopusDeployLabs/octopusdeploy", version = "0.11.1" }
    }
  }</pre>
<p>
    The provider itself must then be defined.
</p>
<p>
    Note that Terraform has rules around where providers can be define. The documentation at <a href="https://developer.hashicorp.com/terraform/language/modules/develop/providers">https://developer.hashicorp.com/terraform/language/modules/develop/providers</a> notes that:
</p>
<p style="margin-left: 2em">
    Provider configurations, unlike most other concepts in Terraform, are global to an entire Terraform configuration and can be shared across module boundaries. Provider configurations can be defined only in a root Terraform module.
</p>
<p>
    This means that when creating a module like <strong>s3backend</strong>, the provider must be defined in the top level module and removed from any child modules.
</p>
<p>
    You will note that the modules under the directories called <strong>octopus</strong> also define a provider. This is because the <strong>octopus</strong> module is assumed to be used directly in the rest of this book. However, the <strong>provider</strong> must be removed from the <strong>octopus</strong> module when using the <strong>s3backend</strong> module.
</p>
<pre>provider "octopusdeploy" {
    address  = "${var.octopus_server}"
    api_key  = "${var.octopus_apikey}"
    space_id = "${var.octopus_space_id}"
  }</pre>
<p>
    The remaining Terraform configuration defines the common variables required by the sibling <strong>octopus</strong> module, and then calls the <strong>octopus</strong> module:
</p>
<pre>variable "octopus_server" {
  type        = string
  nullable    = false
  sensitive   = false
  description = "The URL of the Octopus server e.g. https://myinstance.octopus.app."
}

variable "octopus_apikey" {
  type        = string
  nullable    = false
  sensitive   = true
  description = "The API key used to access the Octopus server. See https://octopus.com/docs/octopus-rest-api/how-to-create-an-api-key for details on creating an API key."
}

variable "octopus_space_id" {
  type        = string
  nullable    = false
  sensitive   = false
  description = "The ID of the Octopus space to populate."
}

module "octopus" {
  source = "../octopus"
  octopus_server = var.octopus_server
  octopus_apikey = var.octopus_apikey
  octopus_space_id = var.octopus_space_id
}</pre>
<p>
    To initialize the <strong>s3backend</strong> module, call <strong>terraform init</strong> with the backend configuration values for your own shared S3 state bucket.
</p>
<p>
    The values for the <strong>bucket</strong> and <strong>region</strong> are the same for all the modules. You must then make sure that each module has a unique value for the <strong>key</strong> setting. 
</p>
<p>
    A Bash script to apply all the modules is shown below. Make sure to replace the values assigned to the <strong>BUCKET_NAME</strong> and <strong>BUCKET_REGION</strong> variables, as well as the variables prefixed with <strong>TF_VAR</strong>:
</p>
<pre>BUCKET_NAME=your_s3_backet_name
BUCKET_REGION=your_aws_region
TF_VAR_octopus_server=https://yourinstance.octopus.app
TF_VAR_octopus_apikey=API-YOURAPIKEY
TF_VAR_octopus_space_id=Spaces-1
TF_VAR_account_aws_account_access=YOUR_AWS_ACCESS_KEY_GOES_HERE
TF_VAR_account_aws_account=YOUR_AWS_SECRET_KEY_GOES_HERE

git clone https://github.com/OctopusSolutionsEngineering/SalesEngineeringAwsLambda.git
cd SalesEngineeringAwsLambda

pushd terraform/feeds/s3backend
terraform init \
    -backend-config="key=feeds" \
    -backend-config="bucket=$BUCKET_NAME" \
    -backend-config="region=$BUCKET_REGION"
terraform apply -auto-approve
popd

pushd terraform/accounts/s3backend
terraform init \
    -backend-config="key=account" \
    -backend-config="bucket=$BUCKET_NAME" \
    -backend-config="region=$BUCKET_REGION"
terraform apply -auto-approve
popd

pushd terraform/environments/s3backend
terraform init \
    -backend-config="key=environment" \
    -backend-config="bucket=$BUCKET_NAME" \
    -backend-config="region=$BUCKET_REGION"
terraform apply -auto-approve
popd

pushd terraform/libraryvariablesets/s3backend
terraform init \
    -backend-config="key=libraryvariablesets" \
    -backend-config="bucket=$BUCKET_NAME" \
    -backend-config="region=$BUCKET_REGION"
terraform apply -auto-approve
popd

pushd terraform/apigateway/s3backend
terraform init \
    -backend-config="key=apigateway" \
    -backend-config="bucket=$BUCKET_NAME" \
    -backend-config="region=$BUCKET_REGION"
terraform apply -auto-approve
popd

pushd terraform/products/s3backend
terraform init \
    -backend-config="key=products" \
    -backend-config="bucket=$BUCKET_NAME" \
    -backend-config="region=$BUCKET_REGION"
terraform apply -auto-approve
popd

pushd terraform/frontend/s3backend
terraform init \
    -backend-config="key=frontend" \
    -backend-config="bucket=$BUCKET_NAME" \
    -backend-config="region=$BUCKET_REGION"
terraform apply -auto-approve
popd</pre>
<p>
    The remainder of this book assumes you are using the default local storage option. Just keep in mind that a shared remote backend is recommended when applying Terraform modules in a production context, and required if executing Terraform in ephemeral environments like Continuous Integration (CI) build agents.
</p>